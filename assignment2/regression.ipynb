{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "regression.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MiqCeFVrfUEW"
      },
      "source": [
        "# Second assignment: Regression\n",
        "\n",
        "Welcome to the second practical assignment! Today, we will learn a bit more about working with data (loading/preprocessing/displaying/\\*ing...) and the first \"real\" model: **Linear Regression**.\n",
        "\n",
        "The purpose of this assignment is to give you a chance to implement your **own regression model**. First, we will start with **Linear Regression with one variable**. As per usual, we will need to deal with some necessary imports and input data (which needs to be downloaded). \n",
        "\n",
        "Will will mostly make use of `numpy`, `matplotlib`, and `scikit-learn` in this exercise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmWtm6ImfUE4"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "!wget -q https://github.com/NaiveNeuron/ml_exercises/raw/master/assignment2/housing_data.csv\n",
        "!wget -q https://github.com/NaiveNeuron/ml_exercises/raw/master/assignment2/food_truck_data.csv\n",
        "!wget -q https://github.com/NaiveNeuron/ml_exercises/raw/master/assignment2/concrete_data.csv\n",
        "!wget -q https://github.com/NaiveNeuron/ml_exercises/raw/master/assignment2/bonus_data.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M358YqUyfUFH"
      },
      "source": [
        "### Part I: Food Truck\n",
        "\n",
        "Imagine you own a food truck and you colected a bunch of data on how well you did (what profit you had) in different cities. You also know the population of these cities. As an efficient owner of a food truck, you would like to maximize your profit and thus set your future path based on the historical data you have. \n",
        "\n",
        "In the variable `data` we loaded exactly this data for you. The first column is the **population** of a city and the second column is the **profit** of a food truck in that city (a negative value for profit indicates a loss). \n",
        "\n",
        "As you can see `numpy` provides a handy function `loadtxt()` (*check out official docs for more info*) that can load almost any type of common data file. Before working with any data, it is usually very useful to see and briefly examine it. A quick way of doing that usually boils down to drawing a plot -- we also do that in the cell below. On X axis we have the population of a given city and on Y axis we have the profit. (*We also suggest you look at the raw data and take a peak inside the actual raw data file.*)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFRExRW-fUFN"
      },
      "source": [
        "data = np.loadtxt('food_truck_data.csv', delimiter=',')\n",
        "\n",
        "plt.scatter(data[:, 0], data[:, 1])\n",
        "plt.xlim(4)\n",
        "plt.xlabel('Population of City in 10 000s')\n",
        "plt.ylabel('Profit in $10 000s')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnJ26ZTcfUFf"
      },
      "source": [
        "Yes, we (the truck driver) live in a very populous (and also rich) country.\n",
        "\n",
        "Now, let's try to fit a *linear regression* model to this data. More formaly, we are going to search for a hypothesis $h_{\\theta} \\in H$ which minimizes the cost function $$J(\\theta) = \\sum_{i=1}^t{err(h_{\\theta}(x_i), y_i)}$$\n",
        "where $t$ is the number of examples, $x_i$ is $i$th example, $y_i$ is $i$th target value, the function $h_{\\theta}$ is described as $$h_{\\theta}(x) = \\theta_0 + \\theta_1x$$ and $err(\\hat{y}, y)$ is an error function which measures the error that the hypothesis made. \n",
        "\n",
        "In our example the error function will be **Mean Squared Error** so we will try to find parameters $\\theta$ that minimize\n",
        "$$J(\\theta) = \\frac{1}{2t}\\sum_{i=1}^t(h_{\\theta}(x_i) - y_i)^2$$\n",
        "\n",
        "You may recall from the lectures that one way of doing this is to use an algorithm called **gradient descent**. In each step of gradient descent you will perform a parameter update so that with each step your parameters will come closer to the optimal values.\n",
        "\n",
        "\n",
        "Putting all of this together, our parameter update now looks as follows:\n",
        "$$\\theta_j = \\theta_j - \\alpha\\frac{1}{t}\\sum_{i=1}^{t}(h_{\\theta}(x_i) - y_i)x_{i,j}$$\n",
        "where $t$ is the number of examples, $\\alpha$ is learning rate, $x_i$ is $i$th example, $y_i$ is $i$th target value and $x_{i, j}$ is $i$th example's $j$th feature.\n",
        "\n",
        "In the next cell you can find a small template, already set up with some important variables. We initilize the values of $\\theta$ to zero, set our learning rate and number of interations. Note that we also put target values into a separate variable `y`. \n",
        "\n",
        "Please finish this setup by preparing the matrix X. It will represent your input matrix where each row is one example. **Remember the intercept term!** (Resulting shape should be (97, 2).)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0qQjQGufUFh"
      },
      "source": [
        "# set learning rate\n",
        "alpha = 0.01\n",
        "# set number of step of gradient descent\n",
        "interations = 1500\n",
        "# target variables\n",
        "y = data[:, 1]\n",
        "\n",
        "# Please, initliaze the input matrix X\n",
        "X = None\n",
        "print(\"Shape of matrix X: {}\".format(X.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPVbHQeOfUFo"
      },
      "source": [
        "Now fill in the body of the function `compute_cost(X, y, theta)` which should compute the cost function described above. Please use vectorized operations rather than loops."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmJjGt1zfUFr"
      },
      "source": [
        "def compute_cost(X, y, theta):\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKq-dBPOfUGD"
      },
      "source": [
        "Next, we will run our gradient descent algorithm. First, we compute an initial cost. Then in each interation we update our paramenters. A sample loop structure is provided, so you only need to supply the parameter update part.\n",
        "\n",
        "If implemented correctly, your cost should steadily go down and never increase."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1ZWwoYmfUGG"
      },
      "source": [
        "# initialize parameters to zero\n",
        "theta = np.zeros((X.shape[1],))\n",
        "\n",
        "def grad_descent(X, y, theta, alpha):\n",
        "    # number of samples\n",
        "    t = len(y)\n",
        "    \n",
        "    print(\"Initial cost: {}\".format(compute_cost(X, y, theta)))    \n",
        "    history = []\n",
        "    \n",
        "    for i in range(interations):\n",
        "        # Update the parameter values on the following line (replace those zeros!)\n",
        "        theta = np.zeros((X.shape[1],))\n",
        "        \n",
        "        cost = compute_cost(X, y, theta)\n",
        "        print(\"Current cost for iteration {}: {}\".format(i, cost))\n",
        "        history.append(cost)\n",
        "        \n",
        "    return theta, history\n",
        "\n",
        "theta, history = grad_descent(X, y, theta, alpha)\n",
        "plt.plot(history)\n",
        "plt.xlabel('Number of iterations')\n",
        "plt.ylabel('Cost')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4M96dXLLfUGc"
      },
      "source": [
        "Now we will use your trained parameters to show the linear line they represent with regards to the training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDuiXB6efUGk"
      },
      "source": [
        "line = np.arange(0, 30)\n",
        "plt.scatter(data[:, 0], data[:, 1])\n",
        "plt.plot(np.array([np.ones(30), line]).T.dot(theta), 'r')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeiw7pnNfUHo"
      },
      "source": [
        "Suppose we have two cities, one with 35000 people and second with 70000 thousand people. What would the predicted profits be for each one of them? ***Please, write your answer below.***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSZ2PsytfUHr"
      },
      "source": [
        "# COMPUTE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWkz3CcbfUIM"
      },
      "source": [
        "*Your answer here.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yRNyrMifUIY"
      },
      "source": [
        "### Part II: Housing\n",
        "\n",
        "In the variable `data` we loaded a new *housing* data for you. The first column in `data` represents the house size (most probably in square feet), the second column represents the number of bedrooms and the last column is the price of the house. As you can see, the house size is generally 1000 times larger that the number of bedrooms. This is undesirable, as  our model will now have to first learn how to scale the data appropriately. It will most probably cause the training time to be longer or it may even make the model fail to find the underlying relationship (distribution) of the data in question. In other words, the training procedure may cause the cost to diverge.\n",
        "\n",
        "This can be solved using **feature normalization**, where we make sure that each feature's value has zero mean and unit variance. Usually, this is done by subtracting the *mean* of the features and and scaling it down by *standard deviation*. The resulting data will have zero mean and unit variance. (People sometimes say they are \"centered\" around zero mean while having unit variance.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAmfhz8-fUIj"
      },
      "source": [
        "data = np.loadtxt('housing_data.csv', delimiter=',')\n",
        "print(\"Shape of loaded data: {}\".format(data.shape))\n",
        "plt.scatter(data[:, 0], data[:, 1])\n",
        "plt.xlabel('House size')\n",
        "plt.ylabel('Number of bedrooms')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piSGdj-1fUI0"
      },
      "source": [
        "Load training data (house size, number of bedrooms) into `X` variable and load train target variables into `y`.\n",
        "\n",
        "It may be that the data you received are ordered (i.e. more expensive houses first, others later). This is also not desired, as we would like the model to rely solely on the data itself (especially in this case) rather than on their order. Shuffling your data should help ensure that (`np.random.permutation` will do the trick in most cases)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lUwNzNtfUI8"
      },
      "source": [
        "X = None\n",
        "y = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVmq_auVfUJO"
      },
      "source": [
        "Now, perform feature normalization by taking the *mean* and *standard deviation* of our dataset. Each feature should have its own *mean* and *standard deviation*. \n",
        "\n",
        "**Note that you need to store these values!** \n",
        "\n",
        "You want to use your model not only on the training data but also on new, previously unseen data. If you only normalized your training data, your model will see the previously unseen data as a completely different distribution and would have tough time handling it (in other words, it would most probably be close to useless). Long story short, you will need to normalize each new example that you would like to predict on using these values. This *mean* and *standard deviation* are becoming parameters and hence crucial parts of your first *machine learning pipeline*.\n",
        "\n",
        "**Do not forget to add intercept term!**\n",
        "\n",
        "\n",
        "#### Do not normalize your target variables! You still want to predict accurate house prices!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyN0_dLUfUJQ"
      },
      "source": [
        "# Normalize features HERE\n",
        "\n",
        "print(\"Training set shape: {}\".format(X.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiKUFWL1fUJb"
      },
      "source": [
        "We will now run the same code as before for optimizing parameters. \n",
        "\n",
        "Your code should already be able to run with arbitrary number of features. If not please rewrite `grad_descent` function with matrix operations so it can handle arbitrary amount of features (and not just one)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O41qf5-TfUJi"
      },
      "source": [
        "# initialize parameters to zero\n",
        "theta = np.zeros((X.shape[1],))\n",
        "alpha = 0.4\n",
        "\n",
        "theta, history = grad_descent(X, y, theta, alpha)\n",
        "plt.plot(history)\n",
        "plt.xlabel('Number of iterations')\n",
        "plt.ylabel('Cost')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTh4hh7ffUJu"
      },
      "source": [
        "Next, please compute the value for house with size of 1650 square feet and 3 bedrooms (It should be around 290000)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgQVuz_HfUJw"
      },
      "source": [
        "# COMPUTE HERE\n",
        "\n",
        "house = None\n",
        "price = None\n",
        "print(\"House price computed by running gradient descent: {}\".format(price))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rutEZmJfUJ4"
      },
      "source": [
        "----------------------------------------------------------------------------------------\n",
        "\n",
        "In the lecture you also learned that the closed-form solution to linear regression can be described as follows:\n",
        "\n",
        "$$\\theta = (X^{T}X)^{-1}X^{T}y$$\n",
        "\n",
        "This form finds exact solution without any iterations, which is pretty nice. (There are some caviats, but let us not talk about those now.)\n",
        "\n",
        "Please provide body of the function `compute_theta_norm_eq` which will return parameters $\\theta$ based on given data in the next cell.\n",
        "\n",
        "\n",
        "Use this function to check if the price of house with 1650 size and 3 bedrooms matches the one found by gradient descent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLP_QFwmfUKD"
      },
      "source": [
        "def compute_theta_norm_eq(X, y):\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VeIV8V71fUKU"
      },
      "source": [
        "theta = compute_theta_norm_eq(X, y)\n",
        "\n",
        "price = None\n",
        "print(\"House price computed by normal equations: {}\".format(price))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1PW1NkyfUKi"
      },
      "source": [
        "If we print raw values of $\\theta$ what can we conclude about our features? Please, provide brief explanations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzrFkefVfUKr"
      },
      "source": [
        "print(theta)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qxUI5s1fULF"
      },
      "source": [
        "#### Write your explation here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-nkP3DsfULR"
      },
      "source": [
        "### Part III: Concrete\n",
        "\n",
        "Again, in `data` variable we have loaded a new dataset for you. This time it is related to compressive strength of concrete. \n",
        "\n",
        "Concrete is the most important material in civil engineering and the concrete compressive strength is a highly nonlinear function of age and ingredients. This data consists out of 8 features: \n",
        "\n",
        "* `(component 1)` Cement  - kg in a m3 mixture\n",
        "* `(component 2)` Blast Furnace Slag  - kg in a m3 mixture\n",
        "* `(component 3)` Fly Ash  - kg in a m3 mixture\n",
        "* `(component 4)` Water  - kg in a m3 mixture\n",
        "* `(component 5)` Superplasticizer  - kg in a m3 mixture\n",
        "* `(component 6)` Coarse Aggregate  - kg in a m3 mixture\n",
        "* `(component 7)` Fine Aggregate  - kg in a m3 mixture\n",
        "* `(component 8)` Age - Day (1~365)\n",
        "\n",
        "and its target variable:\n",
        "\n",
        "* `(component 9)` *Concrete compressive strength - MPa*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VC6_csjzfULT"
      },
      "source": [
        "data = np.loadtxt('concrete_data.csv', delimiter=',')\n",
        "print(\"Shape of concrete compressive strength data: {}\".format(data.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UE7q6Y1gfULc"
      },
      "source": [
        "If there is something we want our model to do, we want it to generalize. To \"learn\" the underlying data distribution from the data it saw, so that we could apply it on data it did not see in trainig. But how do we know how well does our model generalize? \n",
        "\n",
        "For this we usually split any dataset we have into three chunks. One is called **train set** and it is used to train the model. The second one, usually taken from the training set is called **validation set** and it is used to optimize hyperparameters of our model (such as learning rate, form of regularization, strength of regularization, etc...). The last chunk of data is called **test set** and it is used **only once**! It is sometimes called also *hold-out* set because we take the data at the beginning and store it somewhere and use it only in the end to see how well our model can *generalize* on previously unseen data.\n",
        "\n",
        "Please split the concrete data (in the `data` variable) into these three sets. Use the provided variables to store the approperiate sets. Split the data in 60:20:20 ratio (60% for training, 20% for validation set, 20% for test set). Resulting shape should be `(618, 8)`, `(206, 8)` and `(206, 8)`, respectively.\n",
        "\n",
        "We normalize the data for you this time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqyaDk5FfULg"
      },
      "source": [
        "# Shuffle the data\n",
        "\n",
        "X_train = None\n",
        "y_train = None\n",
        "\n",
        "X_val = None\n",
        "y_val = None\n",
        "\n",
        "X_test = None\n",
        "y_test = None\n",
        "\n",
        "print(\"Train set X: {} y: {}\".format(X_train.shape, y_train.shape))\n",
        "print(\"Validation set X: {} y: {}\".format(X_val.shape, y_val.shape))\n",
        "print(\"Test set X: {} y: {}\".format(X_test.shape, y_test.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQPGmVR3fULw"
      },
      "source": [
        "# normalize the data based on traning mean and stadard deviation\n",
        "mean = np.mean(X_train, axis=0)\n",
        "std = np.std(X_train, axis=0)\n",
        "\n",
        "X_train = (X_train - mean) / std\n",
        "X_val = (X_val - mean) / std\n",
        "X_test = (X_test - mean) / std"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tn2BEtCcfUMB"
      },
      "source": [
        "Now we will use the previously constructed function `compute_theta_norm_eq` to compute new parameters for this dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YV681QRvfUMJ"
      },
      "source": [
        "theta = compute_theta_norm_eq(X_train, y_train)\n",
        "print(\"Train MSE error: {}\".format(compute_cost(X_train, y_train, theta)))\n",
        "print(\"Validation MSE error: {}\".format(compute_cost(X_val, y_val, theta)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-i03wunafUMX"
      },
      "source": [
        "As we mentioned before, this data is nonlinear, which means (among other things) that we can not capture the underlining relationship (distribution) with a simple linear function. A high training error as well as high validation error is a good indicator for that as well. This phenomenon is also called **underfitting**. You can sometimes run into the oposite phenomenon called **overfitting**, where the validation error would be high and training error would be low. It often occures when we have a model that is too complex/powerful for our task (i.e. a lot of free parameters for optimization and not too much data to train and test on).\n",
        "\n",
        "However, you may recall from the lectures that linear regression can be generalized to fit more complex functions. We can transform simple linear regression into a more complex model by adding various features to the input vector. For instance, if we would like to have data of more quadratic nature, we can just make new features by squaring  it (that is squaring the input vector)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-B53YGmkfUMZ"
      },
      "source": [
        "On the other hand, if we \"*over-do this*\", we may end up with an input space that the model can capture too easily (and miss the underlying distribution as a result).\n",
        "\n",
        "Suppose we had data that would be of quadratic nature, and we would have a model that would have utilized features all the way up to the power of 9.\n",
        "\n",
        "We would essentially be searching for a good fitting function in vector space of all functions that can be constructed with polynomials of rank 9. Quadratic function can be found in this space (all other constants will be set to 0) but the probability that we will find exactly the right function that we need (in our case quadratic) is quite low. A far more likely scenario is that we will find a function that the model found first, and that this function will be vastly different from that of the underlying distribution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pv_OoLCfUMw"
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "def true_fun(X):\n",
        "    return np.cos(1.5 * np.pi * X)\n",
        "\n",
        "n_samples = 30\n",
        "degrees = [1, 4, 15]\n",
        "\n",
        "X = np.sort(np.random.rand(n_samples))\n",
        "y = true_fun(X) + np.random.randn(n_samples) * 0.1\n",
        "titles = ['underfitting', 'good fit', 'overfitting']\n",
        "\n",
        "plt.figure(figsize=(14, 5))\n",
        "for i in range(len(degrees)):\n",
        "    ax = plt.subplot(1, len(degrees), i + 1)\n",
        "    plt.setp(ax, xticks=(), yticks=())\n",
        "\n",
        "    polynomial_features = PolynomialFeatures(degree=degrees[i],\n",
        "                                             include_bias=False)\n",
        "    linear_regression = LinearRegression()\n",
        "    pipeline = Pipeline([(\"polynomial_features\", polynomial_features),\n",
        "                         (\"linear_regression\", linear_regression)])\n",
        "    pipeline.fit(X[:, np.newaxis], y)\n",
        "\n",
        "    X_test_i = np.linspace(0, 1, 100)\n",
        "    plt.plot(X_test_i, pipeline.predict(X_test_i[:, np.newaxis]),\n",
        "             label=\"Model\")\n",
        "    plt.plot(X_test_i, true_fun(X_test_i), label=\"True function\")\n",
        "    plt.scatter(X, y, edgecolor='b', s=20, label=\"Samples\")\n",
        "    plt.xlabel(\"x\")\n",
        "    plt.ylabel(\"y\")\n",
        "    plt.xlim((0, 1))\n",
        "    plt.ylim((-2, 2))\n",
        "    plt.legend(loc=\"best\")\n",
        "    plt.title(titles[i])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SriciHCafUM4"
      },
      "source": [
        "Your task now is to find good fitting features. Use `X_train` and `y_train` vectors to enroll features to a satisfing rank, so that the training error would be low and validation error would be also low.\n",
        "\n",
        "Please also provide a brief explanation of your setup and approach, and describe in detail when (i.e. at what values of $\\theta$) did you stop experiencing *underfitting* and when did you find you where experiencing *overfitting*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXJJgTclfUM5"
      },
      "source": [
        "# COMPUTE HERE\n",
        "\n",
        "theta = compute_theta_norm_eq(X_train, y_train)\n",
        "\n",
        "print(\"Train MSE error: {}\".format(compute_cost(X_train, y_train, theta)))\n",
        "print(\"Validation MSE error: {}\".format(compute_cost(X_val, y_val, theta)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joj93TC1fUND"
      },
      "source": [
        "**Describe your setup and values of $\\theta$ mentioned above**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIphleFsfUNF"
      },
      "source": [
        "Run you model on the test set when you are **sure**, that you found a good fit (low test error, low validation error)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmIcHH4KfUNG"
      },
      "source": [
        "print(\"Your final test error: {}\".format(compute_cost(X_test, y_test, theta)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bLiph8DfUNM"
      },
      "source": [
        "## For bonus points:\n",
        "\n",
        "\n",
        "One big disadvatage of normal equations is that if we have a big dataset, it is rather impractical to use it for finding $\\theta$. Computing the inverse would take really long time (not to mention how much memory would it need) and therefore we need to use our gradient descent method.\n",
        "\n",
        "In the variables `X` and `y` we loaded some sample data. Please run your implementation of gradient descent algorithm on it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voG15Df6fUNU"
      },
      "source": [
        "data = np.loadtxt('bonus_data.csv', delimiter=',')\n",
        "X = data[:3, :].T\n",
        "y = data[3, :]\n",
        "\n",
        "\n",
        "plt.scatter(X[:, 1], X[:, 2])\n",
        "plt.show()\n",
        "\n",
        "theta = np.zeros((3,))\n",
        "alpha = 0.4\n",
        "\n",
        "theta, history = grad_descent(X, y, theta, alpha)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzfSL76qfUNi"
      },
      "source": [
        "print(\"Your theta coeficients: {}\".format(theta))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CxYVGuFfUNr"
      },
      "source": [
        "To compare the results, please also run the \"out-of-the-box\" implementation of linear regression from `sklearn` library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vv2jk4ebfUNu"
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "linear_regression = LinearRegression()\n",
        "theta = linear_regression.fit(X, y)\n",
        "\n",
        "print(\"scikit-learn's theta coeficients: {}\".format(theta.coef_))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "attLJt22fUOJ"
      },
      "source": [
        "Please, describe what happened.\n",
        "\n",
        "How is this phenomenom called? \n",
        "How can it be resolved?\n",
        "What home-take message do we take from this whole \"bonus\" experience?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJfrgpcCfUOK"
      },
      "source": [
        "*Write your explanation here.*\n",
        "\n",
        "*Please make it brief. We kid you not: more than three sentences and you will have hard time getting any bonus points.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CoaKCbeYfUOM"
      },
      "source": [
        "*If you need any help with this assignment, please feel free to contact the course TAs, whose contact info can be found on the [course website](http://compbio.fmph.uniba.sk/vyuka/ml/).*\n",
        "\n",
        "\n",
        "**Thanks for reading this far!**"
      ]
    }
  ]
}
